{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyfn8McqUEBQ"
   },
   "source": [
    "## CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XfVCwUiETLWX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from multiprocesspandas import applyparallel\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_x264mcXT-5R"
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KpzDtQBUUCd1"
   },
   "outputs": [],
   "source": [
    "topic_df = pd.read_csv('topics.csv')\n",
    "content_df = pd.read_csv('content.csv')\n",
    "corr_df = pd.read_csv('correlations.csv')\n",
    "# topic_df = topic_df.rename(columns={'id': 'topic_id'}).merge(corr_df)\n",
    "topic_df_non_source = topic_df[topic_df['category']!='source'].reset_index(drop=True)\n",
    "topic_df_non_source['stratify'] = topic_df_non_source['category'] + \\\n",
    "topic_df_non_source['language'] + topic_df_non_source['description'].apply(lambda x: str(isinstance(x, str))) + \\\n",
    "topic_df_non_source['has_content'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_lvM_JZuUnoT",
    "outputId": "ee34b1d5-16eb-426c-aa65-5af83917a603"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py:885: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedGroupKFold(n_splits=N_SPLITS)\n",
    "folds = list(kf.split(topic_df_non_source, y=topic_df_non_source[\"stratify\"], groups=topic_df_non_source[\"channel\"]))\n",
    "topic_df_non_source['fold'] = -1\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    topic_df_non_source.loc[val_idx, \"fold\"] = fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "O6YY6bJkUt5H"
   },
   "outputs": [],
   "source": [
    "fold_df =  topic_df.merge(topic_df_non_source[['id', 'fold']], on='id', how='left').reset_index(drop=True)[['id', 'fold']].fillna(-1).rename(columns={'id': 'topic_id'})\n",
    "fold_df['fold'] = fold_df['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e9L5F7uqWhpl"
   },
   "outputs": [],
   "source": [
    "corr_df['content_ids'] = corr_df['content_ids'].apply(lambda x:x.split())\n",
    "corr_df = corr_df.explode('content_ids').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = topic_df.fillna('')\n",
    "topic_df['topic_full_text'] =  topic_df['title'] + ' [SEP] ' + topic_df['description']\n",
    "topic_df = topic_df[['id', 'topic_full_text', 'language']]\n",
    "df = corr_df.merge(topic_df, left_on='topic_id', right_on='id', how='left')\n",
    "df = df[['topic_id','content_ids','topic_full_text','language']]\n",
    "df = df.rename(columns={'language':'topic_language'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DA20jcVaXkkP"
   },
   "outputs": [],
   "source": [
    "content_df = content_df.fillna('')\n",
    "\n",
    "content_df['content_full_text'] =  content_df['title'] + ' [SEP] ' + content_df['description'] + ' [SEP] ' + content_df['text']\n",
    "content_df = content_df[['id', 'content_full_text', 'language']]\n",
    "df = df.merge(content_df, left_on='content_ids', right_on='id', how='left')\n",
    "df = df.rename(columns={'language':'content_language'})\n",
    "df['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "      <th>topic_full_text</th>\n",
       "      <th>topic_language</th>\n",
       "      <th>id</th>\n",
       "      <th>content_full_text</th>\n",
       "      <th>content_language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_1108dd0c7a5d</td>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>bg</td>\n",
       "      <td>c_1108dd0c7a5d</td>\n",
       "      <td>Молив като резистор [SEP] Моливът причинява пр...</td>\n",
       "      <td>bg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_376c5a8eb028</td>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>bg</td>\n",
       "      <td>c_376c5a8eb028</td>\n",
       "      <td>Да чуем променливото съпротивление [SEP] Тук ч...</td>\n",
       "      <td>bg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_5bc0e1e2cba0</td>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>bg</td>\n",
       "      <td>c_5bc0e1e2cba0</td>\n",
       "      <td>Променлив резистор (реостат) с графит от молив...</td>\n",
       "      <td>bg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_76231f9d0b5e</td>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>bg</td>\n",
       "      <td>c_76231f9d0b5e</td>\n",
       "      <td>Последователно свързване на галваничен елемент...</td>\n",
       "      <td>bg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_00068291e9a4</td>\n",
       "      <td>c_639ea2ef9c95</td>\n",
       "      <td>Entradas e saídas de uma função [SEP] Entenda ...</td>\n",
       "      <td>pt</td>\n",
       "      <td>c_639ea2ef9c95</td>\n",
       "      <td>Dados e resultados de funções: gráficos [SEP] ...</td>\n",
       "      <td>pt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id     content_ids  \\\n",
       "0  t_00004da3a1b2  c_1108dd0c7a5d   \n",
       "1  t_00004da3a1b2  c_376c5a8eb028   \n",
       "2  t_00004da3a1b2  c_5bc0e1e2cba0   \n",
       "3  t_00004da3a1b2  c_76231f9d0b5e   \n",
       "4  t_00068291e9a4  c_639ea2ef9c95   \n",
       "\n",
       "                                     topic_full_text topic_language  \\\n",
       "0  Откриването на резисторите [SEP] Изследване на...             bg   \n",
       "1  Откриването на резисторите [SEP] Изследване на...             bg   \n",
       "2  Откриването на резисторите [SEP] Изследване на...             bg   \n",
       "3  Откриването на резисторите [SEP] Изследване на...             bg   \n",
       "4  Entradas e saídas de uma função [SEP] Entenda ...             pt   \n",
       "\n",
       "               id                                  content_full_text  \\\n",
       "0  c_1108dd0c7a5d  Молив като резистор [SEP] Моливът причинява пр...   \n",
       "1  c_376c5a8eb028  Да чуем променливото съпротивление [SEP] Тук ч...   \n",
       "2  c_5bc0e1e2cba0  Променлив резистор (реостат) с графит от молив...   \n",
       "3  c_76231f9d0b5e  Последователно свързване на галваничен елемент...   \n",
       "4  c_639ea2ef9c95  Dados e resultados de funções: gráficos [SEP] ...   \n",
       "\n",
       "  content_language  label  \n",
       "0               bg      1  \n",
       "1               bg      1  \n",
       "2               bg      1  \n",
       "3               bg      1  \n",
       "4               pt      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NhAgbgLbq4SM"
   },
   "outputs": [],
   "source": [
    "df = df[['topic_id', 'topic_full_text', 'content_full_text', 'label']]\n",
    "df = pd.concat([df])\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SoS2ECSO7hfd"
   },
   "outputs": [],
   "source": [
    "df = df.merge(fold_df, left_on='topic_id', right_on='topic_id', how='left')\n",
    "df = df[['topic_full_text', 'content_full_text', 'label' ,'fold']]\n",
    "\n",
    "#df = df[df['fold'].isin([0, 1, 2, 3, 4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "c5XnPHddrrA7"
   },
   "outputs": [],
   "source": [
    "#df.to_csv('train_folds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "flmIFIdcEwkZ",
    "outputId": "70deaf8e-bae0-4110-a9ad-27dbe45a21ab"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    for punctuation in list(string.punctuation): text = text.replace(punctuation, '')\n",
    "    output = re.sub('\\r+', ' ', text)\n",
    "    output = re.sub('\\n+', ' ', output)\n",
    "    \n",
    "    return output\n",
    "df['topic_full_text'] = df['topic_full_text'].apply(lambda x:clean_text(x))\n",
    "df['content_full_text'] = df['content_full_text'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OP_LxmnFb4ry"
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('train_folds.csv')\n",
    "df = df[df['fold'].isin([0, 1, 2, 3, 4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jsi4Pr8wWqM"
   },
   "source": [
    "## create CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Vu2P55dhwp5G"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from transformers import BertTokenizer,AutoModel,AdamW,AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "import random\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CbJxrkWxxWti"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    input_path = '/root/autodl-tmp/'\n",
    "    model_path = 'xlm-roberta-base' \n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5  # 1.5\n",
    "    num_warmup_steps = 0\n",
    "    max_input_length = 300\n",
    "    epochs = 5  # 5\n",
    "    encoder_lr = 20e-6\n",
    "    decoder_lr = 1e-3\n",
    "    min_lr = 0.5e-6\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    weight_decay = 0\n",
    "    num_fold = 5\n",
    "    batch_size = 100\n",
    "    seed = 1006\n",
    "    OUTPUT_DIR = '/root/autodl-tmp/'\n",
    "    num_workers = 2\n",
    "    device='cuda'\n",
    "    print_freq = 100\n",
    "    apex=False\n",
    "    start_awp_epoch = 2 # 开始AWP epoch\n",
    "    adv_lr = 1e-5 # AWP学习率\n",
    "    adv_eps = 1e-3 # AWP epsilon\n",
    "    adv_step = 1 # AWP step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "safR828DxavN"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "j1-oQIqpwalb"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer):\n",
    "        self.topic = df['topic_full_text'].values\n",
    "        self.content = df['content_full_text'].values\n",
    "        self.label = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "    def __len__(self):\n",
    "        return len(self.topic)\n",
    "    def __getitem__(self, item):\n",
    "        topic = self.topic[item].replace('[SEP]', self.sep_token)\n",
    "        content = self.content[item].replace('[SEP]', self.sep_token)\n",
    "        label = int(self.label[item])\n",
    "\n",
    "        \n",
    "        inputs_topic = self.tokenizer(topic, truncation=True, max_length=CFG.max_input_length, padding='max_length')\n",
    "        inputs_content = self.tokenizer(content, truncation=True, max_length=CFG.max_input_length, padding='max_length')\n",
    "        return torch.as_tensor(inputs_topic['input_ids'], dtype=torch.long), \\\n",
    "            torch.as_tensor(inputs_topic['attention_mask'], dtype=torch.long), \\\n",
    "            torch.as_tensor(inputs_content['input_ids'], dtype=torch.long), \\\n",
    "            torch.as_tensor(inputs_content['attention_mask'], dtype=torch.long), \\\n",
    "            torch.as_tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnCXK1kYzPCo"
   },
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "i0w2WY_OzLeC"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "class Custom_Bert_Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path)\n",
    "        self.config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        self.linear = nn.Linear(self.config.hidden_size*3, 1)\n",
    "\n",
    "    def forward(self,\n",
    "        topic_input_ids,\n",
    "        content_input_ids,\n",
    "        topic_attention_mask=None,\n",
    "        content_attention_mask=None, \n",
    "        labels=None):\n",
    "        topic_output = self.base(input_ids=topic_input_ids,attention_mask=topic_attention_mask)\n",
    "        topic_output = topic_output.last_hidden_state\n",
    "        topic_output = torch.mean(topic_output, dim=1)\n",
    "\n",
    "        content_output = self.base(input_ids=content_input_ids,attention_mask=content_attention_mask)\n",
    "        content_output = content_output.last_hidden_state\n",
    "        content_output = torch.mean(content_output, dim=1)\n",
    "\n",
    "        diff = torch.abs(topic_output-content_output)\n",
    "\n",
    "        sentence_embedding = torch.cat([topic_output, content_output, diff], 1)\n",
    "\n",
    "        output = self.linear(sentence_embedding)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.binary_cross_entropy_with_logits(output.view(-1), labels.view(-1))\n",
    "        \n",
    "        return loss, sentence_embedding\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xB4GaI9fDzt1"
   },
   "source": [
    "## build logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "R3SPEdriD2lE"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6VVH3jSEYie",
    "outputId": "b0d4793c-ef1e-4623-b4f7-9a4f6ce628cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===============lr_2e-05===============\n",
      "===============seed_1006===============\n",
      "===============total_epochs_5===============\n",
      "===============num_warmup_steps_0===============\n"
     ]
    }
   ],
   "source": [
    "def get_logger(filename=CFG.OUTPUT_DIR+ 'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "LOGGER.info('===============lr_{}==============='.format(CFG.encoder_lr))\n",
    "LOGGER.info('===============seed_{}==============='.format(CFG.seed))\n",
    "LOGGER.info('===============total_epochs_{}==============='.format(CFG.epochs))\n",
    "LOGGER.info('===============num_warmup_steps_{}==============='.format(CFG.num_warmup_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQDtzCGV5S0B"
   },
   "source": [
    "## build pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adversarial attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    " \n",
    "    def attack(self, epsilon=.01, emb_name='word_embedding'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    " \n",
    "    def restore(self, emb_name='word_embedding'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "def train_fn_adv(train_loader, model, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    fgm = FGM(model)\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = [i.to(device) for i in batch]\n",
    "        topic_input_ids, topic_attention_mask, content_input_ids, content_attention_mask, label = batch\n",
    "        batch_size = label.size(0)\n",
    "        loss = model(topic_input_ids, content_input_ids, topic_attention_mask, content_attention_mask, label)[0]\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 500)\n",
    "        # 对抗训练\n",
    "        fgm.attack() # embedding被修改了\n",
    "        loss_adv =model(topic_input_ids, content_input_ids, topic_attention_mask, content_attention_mask, label)[0]\n",
    "        loss_adv.backward() # 反向传播，在正常的grad基础上，累加对抗训练的梯度\n",
    "        fgm.restore() # 恢复Embedding的参数\n",
    "        # 梯度下降，更新参数\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_loss(p, q, pad_mask=None):\n",
    "    \n",
    "    p_loss = F.kl_div(F.log_softmax(p, dim=-1), F.softmax(q, dim=-1), reduction='none') # b, 36\n",
    "    q_loss = F.kl_div(F.log_softmax(q, dim=-1), F.softmax(p, dim=-1), reduction='none')\n",
    "    \n",
    "    # pad_mask is for seq-level tasks\n",
    "    if pad_mask is not None:\n",
    "        p_loss.masked_fill_(pad_mask, 0.)\n",
    "        q_loss.masked_fill_(pad_mask, 0.)\n",
    "\n",
    "    p_loss = p_loss.sum()\n",
    "    q_loss = q_loss.sum()\n",
    "\n",
    "    loss = (p_loss + q_loss) / 2\n",
    "    return loss\n",
    "\n",
    "def train_fn_r_drop(train_loader, model, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = [i.to(device) for i in batch]\n",
    "        topic_input_ids, topic_attention_mask, content_input_ids, content_attention_mask, label = batch\n",
    "        batch_size = label.size(0)\n",
    "        loss_0,  = model(topic_input_ids, content_input_ids, topic_attention_mask, content_attention_mask, label)\n",
    "        loss_0 = output_0.loss\n",
    "        logits_0 = output_0.logits # batch , num_labels\n",
    "        output_1 = model(input_ids, mask, labels=label)\n",
    "        loss_1 = output_1.loss\n",
    "        logits_1 = output_1.logits\n",
    "        ce_loss = 0.5 * (loss_0 + loss_1)\n",
    "        kl_loss = compute_kl_loss(logits_0, logits_1)\n",
    "        loss = ce_loss + 0.5 * kl_loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 500)\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "S6Pg-_675VB2"
   },
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = [i.to(device) for i in batch]\n",
    "        topic_input_ids, topic_attention_mask, content_input_ids, content_attention_mask, label = batch\n",
    "        batch_size = label.size(0)\n",
    "        loss = model(topic_input_ids, content_input_ids, topic_attention_mask, content_attention_mask, label)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 50000)\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    start = end = time.time()\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        label = batch[2].to(device)\n",
    "        mask = batch[1].to(device)\n",
    "        input_ids = batch[0].to(device)\n",
    "        batch_size = label.size(0)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, mask, labels=label)\n",
    "        loss = output.loss\n",
    "        y_preds = output.logits.argmax(dim=-1)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "        labels.append(label.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    #print(predictions)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "def train_loop(fold, model, train_dataset, valid_dataset):\n",
    "    LOGGER.info(f\"========== training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    #model = Custom_Bert_Simple()\n",
    "    #model = AutoModelForSequenceClassification.from_pretrained(CFG.model_path, num_labels=1)\n",
    "    model.to(CFG.device)\n",
    "\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    def get_optimizer(model):\n",
    "\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                'lr': CFG.encoder_lr, 'weight_decay': CFG.weight_decay},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                'lr': CFG.encoder_lr, 'weight_decay': 0.0}\n",
    "            \n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_parameters, lr = CFG.encoder_lr, eps = CFG.eps, betas = CFG.betas)\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "    optimizer = get_optimizer(model)\n",
    "\n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        cfg.num_warmup_steps = cfg.num_warmup_steps * num_train_steps\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps,\n",
    "                num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "\n",
    "    num_train_steps = int(len(train_dataset) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    # criterion = torch.nn.CrossEntropyLoss(ignore_index=- 1)\n",
    "\n",
    "    # criterion = LabelSmoothingLoss()\n",
    "    best_score = float('inf')\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        #avg_loss = train_fn_awp(train_loader, model, optimizer, epoch, scheduler, CFG.device)\n",
    "        \n",
    "        avg_loss = train_fn(train_loader, model, optimizer, epoch, scheduler, CFG.device)\n",
    "        # eval\n",
    "        #avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, model, CFG.device)\n",
    "\n",
    "        # scoring\n",
    "        #score = get_score(predictions, valid_labels)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(\n",
    "            f'Epoch {epoch + 1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        #LOGGER.info(f'Epoch {epoch + 1} - Score: {score:.4f}')\n",
    "\n",
    "\n",
    "        if best_score > avg_loss:\n",
    "            best_score = avg_loss\n",
    "            #best_predictions = predictions\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save(model.state_dict(),\n",
    "                       CFG.OUTPUT_DIR + \"{}_best{}.pth\".format(CFG.model_path.replace('/', '_'),fold))\n",
    "\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    del scheduler, optimizer, model\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kazL85iWEb5W",
    "outputId": "5953d979-3e8b-4911-8876-834e16ceb357"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "========== training ==========\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/831] Elapsed 0m 1s (remain 26m 51s) Loss: 806.6549(806.6549) Grad: 87543.5391  LR: 0.00002000  \n",
      "Epoch: [1][100/831] Elapsed 1m 12s (remain 8m 43s) Loss: 20.1461(73.3689) Grad: 155.8936  LR: 0.00001997  \n",
      "Epoch: [1][200/831] Elapsed 2m 23s (remain 7m 28s) Loss: 17.0593(45.8183) Grad: 40.3874  LR: 0.00001988  \n",
      "Epoch: [1][300/831] Elapsed 3m 33s (remain 6m 16s) Loss: 15.6878(36.1321) Grad: 18.2892  LR: 0.00001974  \n",
      "Epoch: [1][400/831] Elapsed 4m 44s (remain 5m 4s) Loss: 15.8164(31.1323) Grad: 18.4240  LR: 0.00001954  \n",
      "Epoch: [1][500/831] Elapsed 5m 54s (remain 3m 53s) Loss: 15.6834(28.0694) Grad: 14.4285  LR: 0.00001929  \n",
      "Epoch: [1][600/831] Elapsed 7m 5s (remain 2m 42s) Loss: 15.3419(25.9890) Grad: 10.3700  LR: 0.00001899  \n",
      "Epoch: [1][700/831] Elapsed 8m 15s (remain 1m 31s) Loss: 16.0209(24.5034) Grad: 15.0206  LR: 0.00001863  \n",
      "Epoch: [1][800/831] Elapsed 9m 26s (remain 0m 21s) Loss: 15.5440(23.3798) Grad: 15.3436  LR: 0.00001822  \n",
      "Epoch: [1][830/831] Elapsed 9m 47s (remain 0m 0s) Loss: 15.2303(23.0953) Grad: 22.2415  LR: 0.00001809  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 23.0953  time: 587s\n",
      "Epoch 1 - Save Best Score: 23.0953 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/831] Elapsed 0m 1s (remain 20m 15s) Loss: 14.9996(14.9996) Grad: 16.3990  LR: 0.00001809  \n",
      "Epoch: [2][100/831] Elapsed 1m 11s (remain 8m 37s) Loss: 15.8836(15.4350) Grad: 31.9335  LR: 0.00001762  \n",
      "Epoch: [2][200/831] Elapsed 2m 21s (remain 7m 25s) Loss: 15.2005(15.4547) Grad: 30.5830  LR: 0.00001711  \n",
      "Epoch: [2][300/831] Elapsed 3m 32s (remain 6m 13s) Loss: 15.5030(15.4238) Grad: 25.4832  LR: 0.00001656  \n",
      "Epoch: [2][400/831] Elapsed 4m 42s (remain 5m 2s) Loss: 16.0732(15.3795) Grad: 21.1309  LR: 0.00001597  \n",
      "Epoch: [2][500/831] Elapsed 5m 52s (remain 3m 52s) Loss: 14.7536(15.3508) Grad: 23.9177  LR: 0.00001534  \n",
      "Epoch: [2][600/831] Elapsed 7m 2s (remain 2m 41s) Loss: 15.5580(15.3261) Grad: 38.9713  LR: 0.00001469  \n",
      "Epoch: [2][700/831] Elapsed 8m 13s (remain 1m 31s) Loss: 14.9598(15.3139) Grad: 34.0624  LR: 0.00001401  \n",
      "Epoch: [2][800/831] Elapsed 9m 23s (remain 0m 21s) Loss: 15.2251(15.3329) Grad: 35.1238  LR: 0.00001331  \n",
      "Epoch: [2][830/831] Elapsed 9m 44s (remain 0m 0s) Loss: 15.6571(15.3361) Grad: 42.2672  LR: 0.00001309  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 15.3361  time: 585s\n",
      "Epoch 2 - Save Best Score: 15.3361 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/831] Elapsed 0m 1s (remain 19m 37s) Loss: 15.2550(15.2550) Grad: 47.4542  LR: 0.00001309  \n",
      "Epoch: [3][100/831] Elapsed 1m 11s (remain 8m 38s) Loss: 14.2830(14.9392) Grad: 35.0368  LR: 0.00001236  \n",
      "Epoch: [3][200/831] Elapsed 2m 22s (remain 7m 25s) Loss: 14.2871(14.6457) Grad: 44.5427  LR: 0.00001162  \n",
      "Epoch: [3][300/831] Elapsed 3m 32s (remain 6m 13s) Loss: 12.1099(14.2348) Grad: 136.3542  LR: 0.00001087  \n",
      "Epoch: [3][400/831] Elapsed 4m 42s (remain 5m 2s) Loss: 10.0546(13.5060) Grad: 130.6047  LR: 0.00001011  \n",
      "Epoch: [3][500/831] Elapsed 5m 52s (remain 3m 52s) Loss: 10.9827(12.7933) Grad: 123.9030  LR: 0.00000936  \n",
      "Epoch: [3][600/831] Elapsed 7m 2s (remain 2m 41s) Loss: 10.2919(12.1848) Grad: 311.5224  LR: 0.00000861  \n",
      "Epoch: [3][700/831] Elapsed 8m 12s (remain 1m 31s) Loss: 8.0424(11.7302) Grad: 130.8241  LR: 0.00000786  \n",
      "Epoch: [3][800/831] Elapsed 9m 22s (remain 0m 21s) Loss: 7.8816(11.3165) Grad: 91.4000  LR: 0.00000713  \n",
      "Epoch: [3][830/831] Elapsed 9m 44s (remain 0m 0s) Loss: 7.9364(11.2065) Grad: 88.2170  LR: 0.00000691  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 11.2065  time: 584s\n",
      "Epoch 3 - Save Best Score: 11.2065 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/831] Elapsed 0m 1s (remain 20m 46s) Loss: 8.6012(8.6012) Grad: 93.6070  LR: 0.00000691  \n",
      "Epoch: [4][100/831] Elapsed 1m 11s (remain 8m 37s) Loss: 7.4083(8.0515) Grad: 117.4740  LR: 0.00000620  \n",
      "Epoch: [4][200/831] Elapsed 2m 21s (remain 7m 24s) Loss: 6.1776(7.8014) Grad: 79.3413  LR: 0.00000551  \n",
      "Epoch: [4][300/831] Elapsed 3m 32s (remain 6m 13s) Loss: 7.4478(7.6976) Grad: 115.0042  LR: 0.00000485  \n",
      "Epoch: [4][400/831] Elapsed 4m 42s (remain 5m 2s) Loss: 6.8173(7.5749) Grad: 143.3564  LR: 0.00000422  \n",
      "Epoch: [4][500/831] Elapsed 5m 52s (remain 3m 52s) Loss: 7.3648(7.5072) Grad: 178.0390  LR: 0.00000362  \n",
      "Epoch: [4][600/831] Elapsed 7m 3s (remain 2m 41s) Loss: 8.3927(7.4817) Grad: 107.7244  LR: 0.00000305  \n",
      "Epoch: [4][700/831] Elapsed 8m 13s (remain 1m 31s) Loss: 8.4091(7.4298) Grad: 112.7753  LR: 0.00000253  \n",
      "Epoch: [4][800/831] Elapsed 9m 23s (remain 0m 21s) Loss: 7.3314(7.3719) Grad: 104.4535  LR: 0.00000205  \n",
      "Epoch: [4][830/831] Elapsed 9m 44s (remain 0m 0s) Loss: 7.1816(7.3528) Grad: 117.6742  LR: 0.00000191  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 7.3528  time: 585s\n",
      "Epoch 4 - Save Best Score: 7.3528 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/831] Elapsed 0m 1s (remain 21m 0s) Loss: 8.5523(8.5523) Grad: 102.4305  LR: 0.00000191  \n",
      "Epoch: [5][100/831] Elapsed 1m 11s (remain 8m 38s) Loss: 5.7938(6.7671) Grad: 120.0356  LR: 0.00000149  \n",
      "Epoch: [5][200/831] Elapsed 2m 21s (remain 7m 24s) Loss: 6.6514(6.7210) Grad: 91.3494  LR: 0.00000112  \n",
      "Epoch: [5][300/831] Elapsed 3m 32s (remain 6m 13s) Loss: 7.1213(6.7228) Grad: 134.2628  LR: 0.00000079  \n",
      "Epoch: [5][400/831] Elapsed 4m 42s (remain 5m 3s) Loss: 6.6185(6.7335) Grad: 96.8177  LR: 0.00000053  \n",
      "Epoch: [5][500/831] Elapsed 5m 53s (remain 3m 52s) Loss: 5.8534(6.7059) Grad: 102.6544  LR: 0.00000031  \n",
      "Epoch: [5][600/831] Elapsed 7m 3s (remain 2m 41s) Loss: 7.4289(6.6983) Grad: 108.3129  LR: 0.00000015  \n",
      "Epoch: [5][700/831] Elapsed 8m 13s (remain 1m 31s) Loss: 6.4916(6.6908) Grad: 107.8180  LR: 0.00000005  \n",
      "Epoch: [5][800/831] Elapsed 9m 23s (remain 0m 21s) Loss: 7.0968(6.6988) Grad: 105.1216  LR: 0.00000000  \n",
      "Epoch: [5][830/831] Elapsed 9m 45s (remain 0m 0s) Loss: 7.4129(6.6916) Grad: 136.5051  LR: 0.00000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 6.6916  time: 585s\n",
      "Epoch 5 - Save Best Score: 6.6916 Model\n"
     ]
    }
   ],
   "source": [
    "model = Custom_Bert_Simple()\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n",
    "fold = 0\n",
    "tr_data = df[df['fold']!=fold].reset_index(drop=True)\n",
    "va_data = df[df['fold']==fold].reset_index(drop=True)\n",
    "tr_dataset = TrainDataset(tr_data,tokenizer)\n",
    "va_dataset = TrainDataset(va_data,tokenizer)\n",
    "val_result = train_loop(fold, model,tr_dataset, va_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_b-cAuYKG3Bg"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hnswlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19869/853224595.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGradScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhnswlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hnswlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from transformers import BertTokenizer,AutoModel,AdamW,AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "import random\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    input_path = '/media/will/data/LECR'\n",
    "    model_path = 'microsoft/mdeberta-v3-base' \n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5  # 1.5\n",
    "    num_warmup_steps = 0\n",
    "    max_input_length = 124\n",
    "    epochs = 5  # 5\n",
    "    encoder_lr = 20e-6\n",
    "    decoder_lr = 1e-3\n",
    "    min_lr = 0.5e-6\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    weight_decay = 0\n",
    "    num_fold = 5\n",
    "    batch_size = 32\n",
    "    seed = 1006\n",
    "    OUTPUT_DIR = '/media/will/data/LECR'\n",
    "    num_workers = 2\n",
    "    device='cuda'\n",
    "    print_freq = 100\n",
    "    apex=False\n",
    "    start_awp_epoch = 2 # 开始AWP epoch\n",
    "    adv_lr = 1e-5 # AWP学习率\n",
    "    adv_eps = 1e-3 # AWP epsilon\n",
    "    adv_step = 1 # AWP step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Bert_Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path)\n",
    "        self.config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "\n",
    "    def forward(self,\n",
    "        input_ids,\n",
    "        attention_mask=None):\n",
    "        output = self.base(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        output = output.last_hidden_state\n",
    "        output = torch.mean(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Custom_Bert_Simple()\n",
    "model.load_state_dict(torch.load('LECRmicrosoft_mdeberta-v3-base_best0.pth'),strict=False)\n",
    "model.to(CFG.device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df = pd.read_csv('content.csv')\n",
    "correlations_df = pd.read_csv('correlations.csv')\n",
    "topics_df = pd.read_csv('topics.csv')\n",
    "#topics_df = topics_df[topics_df['category']!='source'].reset_index(drop=True)\n",
    "sub_df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Testataset(Dataset):\n",
    "    def __init__(self,df,tokenizer):\n",
    "        self.title = df['title'].values\n",
    "        self.description = df['description'].values\n",
    "        self.text = None\n",
    "        if 'text' in df.columns:\n",
    "            self.text = df['text'].values\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        input_text = self.title[item]\n",
    "        if isinstance(input_text, float):\n",
    "            input_text = ''\n",
    "        if not isinstance(self.description[item], float):\n",
    "            #print(self.description[item])\n",
    "            input_text += ' ' + self.sep_token + ' ' + self.description[item]\n",
    "        \n",
    "        if self.text is not None and not isinstance(self.text[item], float):\n",
    "            input_text += ' ' + self.sep_token + self.text[item]\n",
    "            \n",
    "        output = self.tokenizer(input_text, truncation=True, max_length=CFG.max_input_length, padding='max_length')\n",
    "        \n",
    "        return torch.as_tensor(output['input_ids'], dtype=torch.long), \\\n",
    "            torch.as_tensor(output['attention_mask'], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dataset = Testataset(topics_df[topics_df['id'].isin(sub_df['topic_id'])], tokenizer)\n",
    "content_dataset = Testataset(content_df, tokenizer)\n",
    "topic_loader = DataLoader(topic_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "content_loader = DataLoader(content_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, dataloader):\n",
    "    res = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids, attention_mask = [i.to(CFG.device) for i in batch]\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask)\n",
    "            res.append(output.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_result = infer(model, topic_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2407/2407 [12:43<00:00,  3.15it/s]\n"
     ]
    }
   ],
   "source": [
    "content_result = infer(model, content_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_ids = [i for i in range(len(content_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(embeddings, ids):\n",
    "\n",
    "    index = hnswlib.Index(space=\"cosine\", dim=embeddings.shape[-1])\n",
    "\n",
    "    # Initializing index\n",
    "    # max_elements - the maximum number of elements (capacity). Will throw an exception if exceeded\n",
    "    # during insertion of an element.\n",
    "    # The capacity can be increased by saving/loading the index, see below.\n",
    "    #\n",
    "    # ef_construction - controls index search speed/build speed tradeoff\n",
    "    #\n",
    "    # M - is tightly connected with internal dimensionality of the data. Strongly affects memory consumption (~M)\n",
    "    # Higher M leads to higher accuracy/run_time at fixed ef/efConstruction\n",
    "    index.init_index(max_elements=embeddings.shape[0], ef_construction=200, M=160)\n",
    "\n",
    "    # Controlling the recall by setting ef:\n",
    "    # higher ef leads to better accuracy, but slower search\n",
    "    index.set_ef(50)\n",
    "\n",
    "    # Set number of threads used during batch search/construction\n",
    "    # By default using all available cores\n",
    "    index.set_num_threads(16)\n",
    "\n",
    "    \n",
    "    index.add_items(embeddings, ids)\n",
    "\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_index = build_index(content_result, content_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = content_index.knn_query(topic_result, k = 5, num_threads = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 107.23it/s]\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "conten_uid = content_df['id']\n",
    "for result in tqdm(results[0]):\n",
    "    top_same = ' '.join(conten_uid[result].to_list())\n",
    "    pred.append(top_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c_9d61ca64065c c_7c38160748ad c_b922de5db068 c_5c0cfe8772fe c_88b54048c6ae'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_9d61ca64065c c_7c38160748ad c_b922de5db068 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00068291e9a4</td>\n",
       "      <td>c_207bb2e7346f c_42c8b513508c c_3a9fabe1f4e0 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_487defefd442 c_b7e629d2a6d0 c_74fc8d315563 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_0006d41a73a8</td>\n",
       "      <td>c_96c5ae7cd9f9 c_aaac446c7b8b c_6953a88de9f6 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_4054df11a74e</td>\n",
       "      <td>c_542e610aa1e1 c_4cc5d89eb9e3 c_2fc11e484b99 c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id                                        content_ids\n",
       "0  t_00004da3a1b2  c_9d61ca64065c c_7c38160748ad c_b922de5db068 c...\n",
       "1  t_00068291e9a4  c_207bb2e7346f c_42c8b513508c c_3a9fabe1f4e0 c...\n",
       "2  t_00069b63a70a  c_487defefd442 c_b7e629d2a6d0 c_74fc8d315563 c...\n",
       "3  t_0006d41a73a8  c_96c5ae7cd9f9 c_aaac446c7b8b c_6953a88de9f6 c...\n",
       "4  t_4054df11a74e  c_542e610aa1e1 c_4cc5d89eb9e3 c_2fc11e484b99 c..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df['content_ids'] = pred\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
